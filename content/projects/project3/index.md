---
title: "DeepZero"
tags: ["Pytorch","DL","MCTS","RL", "C"]
author: ["–û–≥–æ—Ä–æ–¥–Ω–∏–∫–æ–≤ –î–∞–Ω–∏–∏–ª"]
description: "DeepZero" 
summary: "—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ MCTS –¥–ª—è –Ω–∞—Å—Ç–æ–ª—å–Ω—ã—Ö –∏–≥—Ä." 
cover:
    image: "deepzero.png"
    #alt: "Some Uses For Olive Oil"
    relative: true

---
---
![](deepzero.png)

---
##### –°—Å—ã–ª–∫–∏

+ [—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π](https://github.com/Qwest1204/DeepZero)

---
**DeepZero** ‚Äî —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ AlphaZero –¥–ª—è –Ω–∞—Å—Ç–æ–ª—å–Ω—ã—Ö –∏–≥—Ä. –ù–µ–π—Ä–æ—Å–µ—Ç—å –æ–±—É—á–∞–µ—Ç—Å—è –∏–≥—Ä–∞—Ç—å –≤ –∏–≥—Ä—ã –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—É—é –∏–≥—Ä—É (self-play), –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π –∏–ª–∏ –∑–∞—Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)

## üéØ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∏–≥—Ä—ã

| –ò–≥—Ä–∞ | –†–∞–∑–º–µ—Ä –¥–æ—Å–∫–∏ | –î–µ–π—Å—Ç–≤–∏—è | –ö–∞–Ω–∞–ª—ã | –°–ª–æ–∂–Ω–æ—Å—Ç—å |
|------|--------------|----------|--------|-----------|
| ‚ùå‚≠ï –ö—Ä–µ—Å—Ç–∏–∫–∏-–Ω–æ–ª–∏–∫–∏ | 3√ó3 | 9 | 3 | ‚≠ê |
| üî¥üü° –ß–µ—Ç—ã—Ä–µ –≤ —Ä—è–¥ | 6√ó7 | 7 | 3 | ‚≠ê‚≠ê |
| ‚ö´‚ö™ –®–∞—à–∫–∏ | 8√ó8 | 4096 | 5 | ‚≠ê‚≠ê‚≠ê |
| ‚ôüÔ∏è‚ôö –®–∞—Ö–º–∞—Ç—ã | 8√ó8 | 4096 | 13 | ‚≠ê‚≠ê‚≠ê‚≠ê |

## üß† –ê–ª–≥–æ—Ä–∏—Ç–º

DeepZero –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–±–∏–Ω–∞—Ü–∏—é **–≥–ª—É–±–æ–∫–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏** –∏ **–ø–æ–∏—Å–∫–∞ –ø–æ –¥–µ—Ä–µ–≤—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS)**.

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      DeepZero                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   ‚îÇ   –ò–≥—Ä–æ–≤–∞—è   ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   ResNet    ‚îÇ ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ   MCTS     ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ    —Å—Ä–µ–¥–∞    ‚îÇ      ‚îÇ  (policy,   ‚îÇ      ‚îÇ  (–ø–æ–∏—Å–∫)   ‚îÇ  ‚îÇ
‚îÇ   ‚îÇ             ‚îÇ ‚óÄ‚îÄ‚îÄ‚îÄ ‚îÇ   value)    ‚îÇ ‚óÄ‚îÄ‚îÄ‚îÄ ‚îÇ            ‚îÇ  ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

#### 1. üé≤ –ò–≥—Ä–æ–≤—ã–µ —Å—Ä–µ–¥—ã (`games/`)
–ö–∞–∂–¥–∞—è –∏–≥—Ä–∞ —Ä–µ–∞–ª–∏–∑—É–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:

```python
class Game:
    def get_initial_state(self)           # –ù–∞—á–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    def get_next_state(state, action, player)  # –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ö–æ–¥
    def get_valid_moves(state)            # –ú–∞—Å–∫–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Ö–æ–¥–æ–≤
    def check_win(state, action)          # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–±–µ–¥—ã
    def get_value_and_terminated(state, action)  # –ó–Ω–∞—á–µ–Ω–∏–µ –∏ —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å
    def change_perspective(state, player) # –°–º–µ–Ω–∞ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã
    def get_encoded_state(state)          # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
```

#### 2. üß¨ –ù–µ–π—Ä–æ—Å–µ—Ç—å ResNet (`models/resnet.py`)
–û—Å—Ç–∞—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å –¥–≤—É–º—è –≥–æ–ª–æ–≤–∞–º–∏:

```
Input: encoded_state [channels √ó height √ó width]
          ‚îÇ
          ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Conv Block  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  ResBlocks  ‚îÇ √ó N
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚ñº           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Policy ‚îÇ ‚îÇ Value  ‚îÇ
‚îÇ  Head  ‚îÇ ‚îÇ  Head  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ           ‚îÇ
    ‚ñº           ‚ñº
 œÄ(s,a)       v(s)
```

- **Policy Head** `œÄ(s,a)`: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π
- **Value Head** `v(s)`: –û—Ü–µ–Ω–∫–∞ –ø–æ–∑–∏—Ü–∏–∏ [-1, 1]

#### 3. üå≥ MCTS (`models/mcts.py`)
–ü–æ–∏—Å–∫ –ø–æ –¥–µ—Ä–µ–≤—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ —É–ª—É—á—à–∞–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:

```
          Selection          Expansion         Simulation        Backpropagation
              ‚îÇ                  ‚îÇ                  ‚îÇ                   ‚îÇ
              ‚ñº                  ‚ñº                  ‚ñº                   ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ ‚óè ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚óè ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚óè ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚óè ‚îÇ
           ‚îî‚îÄ‚î¨‚îÄ‚îò              ‚îî‚îÄ‚î¨‚îÄ‚îò              ‚îî‚îÄ‚î¨‚îÄ‚îò              ‚îî‚îÄ‚î¨‚îÄ‚îò
           ‚îå‚îÄ‚î¥‚îÄ‚îê              ‚îå‚îÄ‚î¥‚îÄ‚îê              ‚îå‚îÄ‚î¥‚îÄ‚îê              ‚îå‚îÄ‚î¥‚îÄ‚îê
           ‚îÇ   ‚îÇ              ‚îÇ   ‚îÇ              ‚îÇ   ‚îÇ              ‚îÇ   ‚îÇ
          ‚óè   ‚óè              ‚óè   ‚óè              ‚óè   ‚óè‚îÄ‚îÄ‚ñ∂NN        ‚óè   ‚óè
                                  ‚îÇ                  ‚îÇ                 ‚ñ≤
                                  ‚ñº                  ‚ñº                 ‚îÇ
                                  ‚óã              v=0.7 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**UCB —Ñ–æ—Ä–º—É–ª–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —É–∑–ª–∞:**
```
UCB(s,a) = Q(s,a) + C √ó œÄ(s,a) √ó ‚àö(N(s)) / (1 + N(s,a))
```

#### 4. üîÑ Self-Play (`models/deepzero.py`)
–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    –ò—Ç–µ—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  1. Self-Play (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö)                        ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ     ‚îÇ  for game in parallel_games:             ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ      state = initial_state               ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ      while not terminated:               ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ          œÄ = MCTS.search(state)          ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ          action = sample(œÄ)              ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ          memory.append(state, œÄ)         ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ          state = next_state(action)      ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ      assign_values(memory, winner)       ‚îÇ        ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                          ‚îÇ                               ‚îÇ
‚îÇ                          ‚ñº                               ‚îÇ
‚îÇ  2. Training (–æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏)                       ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ     ‚îÇ  for epoch in epochs:                    ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ      for batch in memory:                ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ          œÄ_pred, v_pred = model(states)  ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ          loss = CE(œÄ_pred, œÄ_target)     ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ                + MSE(v_pred, v_target)   ‚îÇ        ‚îÇ
‚îÇ     ‚îÇ          optimizer.step()                ‚îÇ        ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                          ‚îÇ                               ‚îÇ
‚îÇ                          ‚ñº                               ‚îÇ
‚îÇ  3. Save checkpoint                                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
## üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –û–ø–∏—Å–∞–Ω–∏–µ | TicTacToe | Checkers | Chess |
|----------|----------|-----------|----------|-------|
| `num_resBlocks` | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ residual –±–ª–æ–∫–æ–≤ | 4 | 9 | 19 |
| `num_hidden` | –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è | 64 | 128 | 256 |
| `num_searches` | MCTS —Å–∏–º—É–ª—è—Ü–∏–π –∑–∞ —Ö–æ–¥ | 60 | 100 | 400 |
| `num_iterations` | –ò—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è | 3 | 8 | 20 |
| `num_parallel_games` | –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö self-play –∏–≥—Ä | 100 | 32 | 64 |
| `dirichlet_alpha` | –ü–∞—Ä–∞–º–µ—Ç—Ä —à—É–º–∞ –î–∏—Ä–∏—Ö–ª–µ | 0.3 | 0.5 | 0.3 |

## üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ —Ñ–∞–π–ª—ã:
- `model_{Game}_{iteration}.pt` ‚Äî –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
- `optimizer_{Game}_{iteration}.pt` ‚Äî —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞

## üîß API –∏–≥—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥

–í—Å–µ –∏–≥—Ä—ã —Ä–µ–∞–ª–∏–∑—É—é—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:

```python
class Game:
    row_count: int          # –í—ã—Å–æ—Ç–∞ –¥–æ—Å–∫–∏
    column_count: int       # –®–∏—Ä–∏–Ω–∞ –¥–æ—Å–∫–∏
    action_size: int        # –†–∞–∑–º–µ—Ä –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π
    shape_obs: int          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
    
    def __repr__(self) -> str
    def get_initial_state(self) -> np.ndarray
    def get_next_state(self, state, action, player) -> np.ndarray
    def get_valid_moves(self, state) -> np.ndarray
    def check_win(self, state, action) -> bool
    def get_value_and_terminated(self, state, action) -> Tuple[int, bool]
    def get_opponent(self, player) -> int
    def get_opponent_value(self, value) -> int
    def change_perspective(self, state, player) -> np.ndarray
    def get_encoded_state(self, state) -> np.ndarray
    def flip_action(self, action) -> int
```

## üìö –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞

- [Mastering the Game of Go without Human Knowledge](https://www.nature.com/articles/nature24270) ‚Äî AlphaGo Zero
- [A general reinforcement learning algorithm that masters chess, shogi, and Go](https://www.science.org/doi/10.1126/science.aar6404) ‚Äî AlphaZero
- [Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265) ‚Äî MuZero


## ü§ù –í–∫–ª–∞–¥

Pull requests –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é—Ç—Å—è! –î–ª—è –∫—Ä—É–ø–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å–Ω–∞—á–∞–ª–∞ –æ—Ç–∫—Ä–æ–π—Ç–µ issue.

---

<p align="center">
  Made with ‚ù§Ô∏è and üß†
</p>