---
title: "Роботы"
tags: ["Pytorch","ML","CS","JAX", "C"]
author: ["Огородников Даниил"]
description: "Модель сжатия с использованием механизмов внимания для неструктурированных данных" 
summary: "Модель сжатия с использованием механизмов внимания для неструктурированных данных" 
cover:
    image: "text2action.png"
    #alt: "Some Uses For Olive Oil"
    relative: true

---
![](text2action.png)

---
#### Интро
Современные архиваторы вроде gzip имеют непредсказуемую эффективность (сжатие в 2–5 раз для текста, 0–10% для шифрованных данных) и требуют много времени (5 мин на 10 ГБ).

#### Задача
Основной задачей я поставил перед собой создание универсального алгоритма для сжатия бинарных данных на базе трансформера вроде BERT, чтобы сжимать с минимальными потерями и полным восстановлением (по крайней мере попытаться).

#### Основные сложности

* Огромная сложность O(n^2) для длинных последовательностей до 2^15 байт, которая потребляет кучу памяти (>32 ГБ) и замедляет обучение.
* Фиксированный контекст в трансформерах не даёт работать с супердлинными данными без переобучения. 
* Нужно гарантировать точное восстановление после удаления кусков, без ошибок в генерации.

#### Мой подход

Механизм внимания на латентном пространстве снижает память до 1.2 ГБ для 128k токенов (256 КБ) и ускоряет обучение в 3.2 раза. Динамическое позиционное кодирование устраняет лимиты фиксированного контекста без переобучения.

#### Алгоритм: 
##### сжатие
1. Чтение метаданных
2. Разбиение на чанки
3. Компрессия чанка: Прореживание токенов (удаение n байт каждые n^2 байт)

Потенциальные проблемы: lossy-эффект — skipped части теряются, если модель не восстановит идеально. Для критических метаданных метод не применяется, чтобы избежать порчи файла.

##### восстановление
1. Нейронка распаковывает сжатый файл и сразу считает его "отпечаток" (хеш)
2. Если отпечаток не совпадает с оригиналом, мы измеряем разницу в латентном пространстве и слегка подправляем результат.
3. Повторяем пару раз — и файл становится идеально точным, как был.

#### Успехи
weissman score 2.981 для файла в 10ГБ

#### Дорабатываю

* переписываю модель с pytorch на jax для ускорения обучения и инференса
* дорабатываю архетектуру, изучаю более эффективные архетектуры трансформера