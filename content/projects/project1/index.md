---
title: "Сжимаем файлы"
tags: ["Pytorch","ML","CS","JAX", "C"]
author: ["Огородников Даниил"]
description: "Модель сжатия с использованием механизмов внимания для неструктурированных данных" 
summary: "Модель сжатия с использованием механизмов внимания для неструктурированных данных" 
cover:
    image: "fileformer.png"
    #alt: "Some Uses For Olive Oil"
    relative: true

---
![](fileformer.png)

---
##### Ссылки

+ [текст работы](FileFormer-word.pdf)

---
### Интро
Традиционные архиваторы (7-Zip, Zstandard, Brotli) дают предсказуемо ограниченную степень сжатия на произвольных бинарных данных и требуют значительных ресурсов уже на этапе упаковки.  
Нейросетевые компрессоры (NNCP, LMC, ZipNN) показывают отличные результаты, но GPU-зависимы в обе стороны — и при сжатии, и при распаковке.

FileFormer решает проблему радикально асимметричным подходом:
- сжатие полностью детерминированное, работает на CPU за секунды даже на терабайтах,
- восстановление — только по запросу, с помощью трансформера с латентным вниманием.

### Задача
Создать универсальный архиватор, который:
- сжимает любые файлы (исполняемые, изображения, базы, зашифрованные данные) на 50–90 % и лучше,
- упаковывает данные быстрее, чем 7-Zip -9,
- позволяет выполнять семантический и контентный поиск по архиву без полной распаковки,
- гарантирует контролируемое восстановление (≥ 95 % совпадения эмбеддингов хешей, в большинстве случаев 100 % бит-в-бит).

### Основные решенные сложности
- Длина чанка 256 КиБ (262 144 токена) — классическое внимание O(n²) невозможно.  
  Решение: Multi-Head Latent Attention (Latte/MLA) → потребление памяти ~1.4 ГБ на чанк в FP16, инференс на одной RTX 4090.
- Точное восстановление после агрессивного удаления байтов.  
  Решение: CRC32-хеш чанка + несжатые метаданные как сильное условие + итеративная коррекция по эмбеддингу хеша.
- Поиск по огромным архивам.  
  Решение: встроенная векторная база (Qdrant/LanceDB) с эмбеддингами чанков и метаданных.

### Алгоритм

#### Сжатие
1. Чтение несжимаемых метаданных
2. Разбиение файла на чанки ровно 256 КиБ
3. Вычисление CRC32 для каждого чанка
4. Применение настраиваемого правила прореживания (например, «оставить 3 байта → удалить 2»)
5. Запись в архив: метаданные + правило CRC32-хеши сжатые данные

#### Восстановление (только при распаковке)
1. Вставка токена MASK на места удалённых байтов
2. Инъекция эмбеддингов CRC32 и метаданных
3. Первый проход трансформера с латентным вниманием
4. Сравнение эмбеддинга хеша восстановленного чанка с оригиналом
5. Итеративная коррекция до ≥ 95 % (обычно 1–3 прохода → 100 % бит-в-бит)

#### Поиск
Автоматическая индексация всех чанков и метаданных в векторную базу → ANN-поиск за миллисекунды даже по 100+ ТБ архивам.

### Текущий статус (декабрь 2025)
- Готово ядро на Rust (упаковка/распаковка без НС, CLI, формат .ff)
- Реализована полная архитектура трансформера с Multi-Head Latent Attention
- Проведены теоретические расчёты памяти и FLOPs
- Идёт сбор 50+ ТБ реального корпуса и подготовка к масштабному обучению FileFormer-Large