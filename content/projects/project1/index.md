---
title: "Сжимаем файлы"
tags: ["Pytorch","ML","CS","JAX", "C"]
author: ["Огородников Даниил"]
description: "Модель сжатия с использованием механизмов внимания для неструктурированных данных" 
summary: "Модель сжатия с использованием механизмов внимания для неструктурированных данных" 
cover:
    image: "fileformer.png"
    #alt: "Some Uses For Olive Oil"
    relative: true

---
![](fileformer.png)

---

#### Задача
Основной задачей я поставил перед собой создание универсального алгоритма для сжатия бинарных данных на базе трансформера вроде BERT, чтобы сжимать с минимальными потерями и полным восстановлением (по крайней мере попытаться).

#### Основные сложности

* Огромная сложность O(n^2) для длинных последовательностей до 2^15 байт, которая потребляет кучу памяти (>32 ГБ) и замедляет обучение.
* Фиксированный контекст в трансформерах не даёт работать с супердлинными данными без переобучения. 
* Нужно гарантировать точное восстановление после удаления кусков, без ошибок в генерации.

#### Мой подход

* Я собрал архитектуру из 16 слоёв с локальным вниманием (окно 64 токена, O(n·k)), двухслойным FFN (2048 hidden, GELU), нормализацией и остатками;
* эмбеддинг 768, динамические позиции для длинных seq;
* Оптимизировал: память до 4.1 ГБ, обучение в 3.2 раза быстрее, mixed-precision на A100. 

#### Алгоритм: 
1. файлы в tarball с 512-бит хешем и именем; 
2. удаляю каждые n байт блок n² (n в хедере);
3. декомпрессия — маски <|mask|>, генерирую на embeddings хеша/имени, итеративно до матча хеша. Плюс векторизация для молниеносного поиска по мета.

###### Признаки по конкретному промокоду
минимальная сумма заказа, размер скидки, подходит ли промокод под доставку, «лучше ли» текущий промокод, чем предыдущие у этого клиента (по скидке и по минимальной сумме).

###### Мобильные события
простые счётчики: количество открытий приложения, просмотров меню, добавлений в корзину за последние 7/14/30 дней.

###### Моделирование
* LinearRegression как быстрый и удивительно сильный бейзлайн (ROC-AUC ≈ 0.78 на кросс-валидации).
* XGBoost с тщательно подобранными гиперпараметрами и early stopping (max_depth=3, subsample=0.9, colsample_bytree=0.5, scale_pos_weight, learning_rate ≈ 0.04) — лучший одиночный результат ≈ 0.882 на валидации.
* CatBoost дал чуть хуже (~0.86), но использовался для ансамбля.
Итоговый скор на привате получен усреднением 5-fold XGBoost моделей.