---
title: "Сжимаем файлы"
tags: ["Pytorch","ML","CS","JAX", "C"]
author: ["Огородников Даниил"]
description: "Модель сжатия с использованием механизмов внимания для неструктурированных данных" 
summary: "Модель сжатия с использованием механизмов внимания для неструктурированных данных" 
cover:
    image: "fileformer.png"
    #alt: "Some Uses For Olive Oil"
    relative: true

---
![](fileformer.png)

---
#### Интро
Главная проблема систем сжатия - Современные архиваторы, такие как gZIP ограничены непредсказуемой эффективностью (2–5 раз для текста, 0–10% для шифрованных данных) и высокими затратами времени сжатия (5 минут для 10 ГБ).

#### Задача
Основной задачей я поставил перед собой создание универсального алгоритма для сжатия бинарных данных на базе трансформера вроде BERT, чтобы сжимать с минимальными потерями и полным восстановлением (по крайней мере попытаться).

#### Основные сложности

* Огромная сложность O(n^2) для длинных последовательностей до 2^15 байт, которая потребляет кучу памяти (>32 ГБ) и замедляет обучение.
* Фиксированный контекст в трансформерах не даёт работать с супердлинными данными без переобучения. 
* Нужно гарантировать точное восстановление после удаления кусков, без ошибок в генерации.

#### Мой подход

Механизм внимания используя латентное пространство обеспечивает снижение потребления памяти до 1.2 ГБ при длине последовательности в 128тыс токенов(256kb) и ускорение обучения в 3.2 раза. Механизм динамического позиционного кодирования устраняет ограничения фиксированной длины контекста без переобучения модели.

#### Алгоритм: 
1. Нейронка распаковывает сжатый файл и сразу считает его "отпечаток" (хеш)
2. Если отпечаток не совпадает с оригиналом, мы измеряем разницу в латентном пространстве и слегка подправляем результат.
3. Повторяем пару раз — и файл становится идеально точным, как был.

#### Успехи
weissman score 2.981 для файла в 10ГБ

#### Дорабатываю

* переписываю модель с pytorch на jax для ускорения обучения и инференса
* дорабатываю архетектуру, изучаю более эффективные архетектуры трансформера
* 